---
title: "Linear Regressions with Python"
author: "Paulo Gugelmo Cavalheiro Dias"
date: 07/05/2024
categories:
 - python
 - statistics
engine: python3
draft: false
---

In this article, I will try to do linear regressions with Python. 
This is also an occasion for me to try Python with Quarto. 

# 1. Quarto configuration for Python

This section is dedicated to using Python with [Quarto](https://quarto.org), since this website was done using Quarto (if you want to know more about it, you can check [their documentation here](https://quarto.org/docs/websites/) and [this project of mine](https://www.paulogcd.com/portfolio_tutorial/)).
If you do not use Quarto, you can just skip this section. 

The Quarto Markdown format (with the .qmd extension) allows to embed code and text, formatted with Markdown.
To work with Python code in Quarto, we first have to use an engine that supports Python. 
For that, we can use `python3` or `jupyter` for examples. 
Thus, in our `YAML` header, we would enter : 

```{.yaml}
---
engine: python3
# or : 
engine: jupyter
---
```

We can check our engines with quarto with the command `quarto check`. 
This function is designed to "verify [the] correct functioning of Quarto installation" (from its description in documentation).
Running it returns the versions of different kernels that we can use.

Finally, to include python code in our Quarto file, we can write : 

````
```{{python}}
# Some python code here
```
````

For example : 

```{python}
# Getting the version of Python we are using : 
import sys 
sys.version_info
```

Here, we are for example using Python 3.12.

## 1.1. Running Julia and R in the same document 

Depending on the engine we are using, we can also make use of other languages in the same Quarto document. 
Specifying the `python3` engine in the `YAML` header of our Quarto document, we can run Julia and R in it.
For that, we need packages, such as `JuliaCall` for using Julia with R.

Testing Julia yields : 
```{julia}
# Getting the version of Julia we are using : 
using InteractiveUtils
InteractiveUtils.versioninfo()
```

Testing R yields : 
```{R}
# Getting the version of R we are using : 
version
```

Since the rest of the article will exclusively be using Python, we will not mention further details of engines in Quarto.
It is however interesting to keep in mind that Quarto allows this multiple integration.

# 2. Simple linear regression

This section of the article treats how to do a simple linear regression with Python. 
We will first generate random data, we will plot it, and we will then add the linear regression to our plot. 

## 2.1. Generating data

I found two main libraries to generate random data in Python. 
The first one is the `random` library, and the second one is the `numpy` library.

With the random library, we can use the `random` method to generate by default a float number between 0 and 1. 
A way to create a vector of 100 random values is to use the `random` method between square brackets, followed by the `for` instruction, similar to the one in Julia. 
This will generate a list of size fixed, such that : 

```{python}
import random
y = ([random.random() for _ in range(100)])
x = ([random.random()*y[_] for _ in range(100)])
```

Let us comment a bit this code : 

- It is possible to call methods with the name of their libraries in front of it.
In this sense, `random.random()` refers to the method `random()` inside the `random` library.
- The underscore syntax `_` combined with `for` loops to iterated for a set number of times. 
By default, the `_` symbol just refers to the last variable in memory of Python, but it can also be used like that. 
- The `range()` function allows to create a range object in Python. More specifically : 

> [It] return[s] an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step.
Range(i, j) produces i, i+1, i+2, ..., j-1. Start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.
These are exactly the valid indices for a list of 4 elements.
When step is given, it specifies the increment (or decrement).

A second way of generating random data is using the numpy library. 
For that, we use the `numpy.random.rand()` method : 

```{python}
import numpy
numpy.random.rand(100)
```

Here, the `numpy.random.rand()` method also generates a random real number between 0 and 1. 
We are going to stick with the first x variable created for the rest of the section.

## 2.2. Plotting the data

Now that we generated some data, let us plot it.
To do that, we can use the [matplotlib library](https://matplotlib.org/). 
More specifically, we are going to use pyplot inside matplotlib, and are thus going to import `matplotlib.pyplot` :

```{python}
import matplotlib.pyplot
matplotlib.pyplot.scatter(x,y)
```

- The `scatter()` method of `matplotlib.pyplot` allows to plot a scatterplot with two vectors of the same size.

> The `scatter()` "method" can also be considered as a function.
This distinction seems to also exist in other programming languages and is definitely worth understanding. 
For the sake of linear regressions, it does not matter for now to develop a full understanding of this distinction.
In the rest of this article, we will thus stick to the "method" term without further explanation. 

## 2.3. Fitting our model

We are now going to fit our statistical model with our randomly generated values. 
In order for us to do that, we can use the `scipy.stats` library.
This library has a `linregress()` method that allows to compute the OLS estimates to explain the second variable with the first one : 

```{python}
import scipy
model = scipy.stats.linregress(x,y)
model
```

We see that the `scipy.stats.linregress()` method returns a five elements object.
To make this method easier to use, we could generate five variables, define a function with those variables and plot a line based on this function : 

```{python}
# We define the five variables from the linregress() method :
slope, intercept, r, p, std_err = scipy.stats.linregress(x, y)

# We define a function returning a linear function with the slope and the intercept :
def myfunc(x):
  return slope * x + intercept

# We create a list object with the list() function that takes as an argument a map object, based on our function and on the variable x :
mymodel = list(map(myfunc, x))
mymodel
```

The definition of the variable `mymodel` is worth explaning a bit more in details.
The `list()` function is used to create a `list` object (which is considered one of the most versatile objects in Python), like the `c()` function creates a vector in R.
The `map()` function returns a map object, defined as : 

> map(func, *iterables) --> map object
Make an iterator that computes the function using arguments from each of the iterables.  Stops when the shortest iterable is exhausted.

in the Python documentation. 

Now, if we want to add this line to our plot, we can run : 

```{python}
matplotlib.pyplot.scatter(x,y)
matplotlib.pyplot.plot(x, mymodel)
```

While it is curious that the variable mymodel is displayed as a continuous line, it works, so I am fine with this solution for now.

If we want only the more relevant information, we can choose to only work with the two first elements of the array, and plot it, we can do in a more concise way :

```{python}
def f(x):
    return scipy.stats.linregress(x, y)[1]+numpy.dot(x,scipy.stats.linregress(x, y)[0])

float(scipy.stats.linregress(x, y)[1]),float(scipy.stats.linregress(x, y)[0])
matplotlib.pyplot.scatter(x,y)
matplotlib.pyplot.plot(x, f(x))
```

We note here that we are using the `numpy.dot()` method from the `numpy` library. 
This method allows us to do the dot product of two different matrixes.
In our case, doing only `x * scipy.stats.linregress(x,y)[0]` would have returned an error, since `x` and the other term are not of the same dimension.
This method thus allows to avoid this error by vectorizing the multiplication. 

Now that we covered a how to perform a simple linear regression with Python, let us move to a multiple linear regression.

# 3. Multiple linear regression

In this section, we are going to perform a multiple linear regression. 
First, we are going to generate data and fit our model.
Then, we will plot a static scatter plot, and finally a 3D rotatable plot.

## 3.1. Running a multiple linear regression

First, let us generate some random data and fit our model. 

To generate random data, we can do : 

```{python}
# We randomly generate four variables of length 100 :
w,x,y,z = (numpy.dot([random.random() for _ in range(100)],_) for _ in (1,2,3,4))
# Just to check the length : 
len(y)
```

To run a multinomial linear regression that satisfies the OLS equation, we could use several libraries. 
Let us say that we want to explain the `z` variable by x and y.

### 3.1.1. The sklearn library

One approach that seems to be widely used is to use the `sklearn` library. 
More specifically, we could use the `sklearn.linear_model` package :  

```{python}
import sklearn.linear_model

# We create an object that regroups x, y and w :
regressors = numpy.column_stack((x,y,w))

# We create the object 'model' that has the LinearRegression() class from the sklearn.linear_model package : 
model = sklearn.linear_model.LinearRegression()
 
# Fit the model to the data
model.fit(regressors, z)

model.coef_
model.intercept_
```

The model.intercept_ value is the intercept of z, and the coefficients are consistent with the regressors object. 
The first one corresponds to the effect of x, the second to the effect of y, and the last one to the effect of w. 

### 3.1.2. The statsmodels library

To get a nicer display and final object, we can alternatively use the statsmodels library.

```{python}
import statsmodels.api

# Create the object and specify an intercept :
regressors = statsmodels.api.add_constant(regressors)
model = statsmodels.api.OLS(z,regressors)

# Run the model :
results = model.fit()

# Print the regression table : 
print(results.summary())
```

We can also access to the itnercept and the coefficients directly, by doing : 
```{python}
results.params
```

## 3.2. Plotting a static 3D plot

If we now want to limit ourselves to a four dimensional problem, we can plot 3D graphs with colors representing the fourth dimension. 
In this section, we are first going to plot a static graph. 

```{python}
# We first create the framework of a plot, that we name "figure" : 
figure = matplotlib.pyplot.figure()

# In this figure, we are going to use the add_subplot method to create a three dimensional projection : 
projection = figure.add_subplot(projection='3d')

# In this three dimensional space, we are going to project our points in a scatterplot way.
# The argument 'c' stands for the color of the points :
projection.scatter(x,y,z, c = w)

# Finally, we give each of our axis a name : 
projection.set_xlabel("X")
projection.set_ylabel("Y")
projection.set_zlabel("Z")
```

The syntax of this example can be a bit hard to understand at first, but is required when using `matplotlib.pyplot` library.

## 3.3. Plotting a rotatable 3D plot 

Now it's time for a rotatable plot, like in the previous article. 
Fortunately, the plotly library does also exist for Python, which is great, because I did not find any other library that yielded a similar result. 
However, to use plotly, we need the pandas library.  
We can thus create a dataframe object from a dictionary with the four previously created vectors. 
If we only want to display a 3D scatter plot, we can run : 

```{python}
import plotly.express
import pandas

# We create a four columns dataframe with our four previously generated vectors : 
data = {'c1':[x],'c2':[y],'c3':[z],'c4':[w]}
X = pandas.DataFrame(data)

fig = plotly.express.scatter_3d(X, x,y,z, color=w)
fig.show()
```

We note that this yields a plot with similar features to the ones obtained in [the previous article with Julia](https://www.paulogcd.com/blog/posts/1_regressions_julia/). 
The reason for that is that both obtained plots are done using the plotly library. 

For the rest of the section, we are going to limit ourselves to only two regressands, excluding the w variable, since the display will be easier to understand this way : 

```{python}
X = numpy.column_stack((x,y))
X = statsmodels.api.add_constant(X)
model = statsmodels.api.OLS(z,X)
results = model.fit()
print(results.summary())
```

Now, we need more lines to plot a surface. 
More specifically, we are going to use plotly.graph_objects and make use of the meshgrid method : 

```{python}
import plotly.graph_objects

# We first store the information of our regression : 
intercept, coef1, coef2 = results.params

# We then create meshgrids for x,y, and z : 
x_range = numpy.linspace(x.min(), x.max(), 10)
y_range = numpy.linspace(y.min(), y.max(), 10)
x_grid, y_grid = numpy.meshgrid(x_range, y_range)
z_grid = intercept + coef1 * x_grid + coef2 * y_grid

# We now generate a surface object with the grids we just created
surface = plotly.graph_objects.Surface(
    x=x_grid, y=y_grid, z=z_grid,
    colorscale='Viridis', opacity=0.5
)

# We create a scatter object, to be able to plot it with the surface :
scatter = plotly.graph_objects.Scatter3d(
    x=x, y=y, z=z,
    mode='markers',
    marker=dict(size=5, color=w, colorscale='Viridis')
)

# We create a figure object, with the scatter and surface tha we just generated : 
fig = plotly.graph_objects.Figure(data = [scatter, surface])

# We display the figure : 
fig.show()
```

This looks satisfactory. 
I even manage to plot the points with the surface of the regression in the 3D plot. 
