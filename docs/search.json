[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Linear Regressions with Python\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regressions with Julia\n\n\n\n\n\n\njulia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose statement\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/purpose_statement/index.html",
    "href": "posts/purpose_statement/index.html",
    "title": "Purpose statement",
    "section": "",
    "text": "Hello, I am Paulo Gugelmo Cavalheiro Dias, currently a graduate level economics students, and this is the first post of my blog, in which I would like to write its purpose statement.\nI want this blog to be a way of sharing my progress in the topics I am interested in, i.e. economics and related subjects, such as programming for social sciences, environmental economics, etc. To do so, I will try to write regular reports on my work in social science, about my classes, readings and other materials."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html",
    "href": "posts/1_regressions_julia/index.html",
    "title": "Linear Regressions with Julia",
    "section": "",
    "text": "The first time I heard about Julia was a few months ago, in an introductory class to programming. Since then, I have come to know a bit more about the reputation of the language, supposely very efficient and especially suited for scientific computations.\nIn this article, I will try to perform some basic linear regressions with Julia and to plot them."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html#simple-linear-regression",
    "href": "posts/1_regressions_julia/index.html#simple-linear-regression",
    "title": "Linear Regressions with Julia",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nHere, let us try to explain one continuous variable by another continuous variable.\nFirst, those are the packages we are going to use :\n\nusing Plots\nusing GLM\n\n\nThe Plots package seems to be the most general package to plot data with Julia. It is a simple interface to several underlying plotting packages, like gr or plotly. In this sense, it allows to easily switch between different frameworks that have different features, while keeping the same syntax.\nThe GLM package seems to be the main package for Generalized Linear Models (GLM) with Julia. It will be useful in the computation of our simple and multinomial linear models.\n\nNow, let us generate some data :\n\nx = range(0, 1, length=100)\n# Generate data : \ny = rand(100) .* x\n\n# Check the data :\nsize(y)\n\n(100,)\n\n\n\nThe function range() in Julia allows to create an array with a starting and an ending value. Although having similar properties than vectors, arrays created with the range() function have the “Range” type (or StepRangeLen to be exact). Here, I just initialise the x array to compute an easy to use y vector.\nThe function rand() in Julia allows to randomly generate numbers. The default settings of the function make it so that the drawn data is of type Float64 between \\(0\\) and \\(1\\). Its argument, set to the value of \\(100\\), indicates the number of random values that are to be drawn. Therefore, rand(100) returns a vector of 100 Float64 numbers.\nThe .* function is a vectorized multiplication. In Julia, adding a dot . before a mathematical operator allows to apply this operation to all the elements of a vector. If we wanted to add \\(2\\) to all the elements of the y vector, we should have written y .+= 2.\nThe last function size() is useful to get more dimension of the array we consider. In our case, it yields (100,) because it is a \\(100\\) rows vector. We would get the same if we did size(y).\n\nNow, let us plot the data we just created :\n\nplot(\n    scatter(x,y, label = \"data\"),\n    title = \"Generating Random Data\",\n    ylabel = \"Random variable\",\n    xlabel = \"x\",\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function plot() in Julia comes from the Plots package. It is the default plotting function, with the gr backend by default (we will get to that later on). It contains several elements :\n\nThe function scatter() allows to plot a scatterplot, i.e. independent points that are not linked by a line. If we had not indicated the scatter() function, it would have plotted a line between all the points.\nThe label value is the one that gets displayed within the box. Specifying “data” allows to avoid the “y1” default value.\ntitle, ylabel and xlabel are pretty straightforward to understand.\n\nIt is now time to run a simple linear regression. Two simple ways to run such a model are :\n\nUsing the GLM package,\nUsing the built-in matrix computation function of Julia.\n\n\nGLM package\nFirst, with the GLM package :\n\nmodel = lm(@formula(y ~ x), (;y, x))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int32}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     t  Pr(&gt;|t|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.00995436   0.03272    0.30    0.7616  -0.0549775  0.0748862\nx            0.468727     0.0565302  8.29    &lt;1e-12   0.356545   0.58091\n──────────────────────────────────────────────────────────────────────────\n\n\nNow, if we want to plot our model with the data, we can run :\n\ncoefs = GLM.coef(model)\nplot!((x) -&gt; coefs[1] + coefs[2] * x, label=\"model\", \n    title = \"Simple linear regression\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to include the equation of our linear model in the legend of the plot, we can use the LaTeXStrings.jl package :\n\nusing LaTeXStrings\nplot(title = \"Simple linear regression\", \n    scatter(x,y, label = false),\n    (x) -&gt; coefs[1] + coefs[2] * x, label=latexstring(\"y=$(coefs[1]) + $(coefs[2]) * x\"))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we want it to be outside the plot :\n\nplot!(legend=:outerbottom, legendcolumns=1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix computation\nSecond, with the built-in matrix computation function :\n\nX = [ones(length(x)) x] \n(X\\y)\n\n2-element Vector{Float64}:\n 0.00995436086366171\n 0.4687272109439466\n\n\nThe first line creates a matrix \\(X\\) such that \\(X\\in\\mathbb{R}^{100\\times 100}\\), with its first columns being only ones, and its second column being the vector of \\(x\\).\nWithout entering into the details, it does exactly what the OLS method does, but in a more rudimentary way, using the normal equation to find the OLS estimates.\n\n\nPerformance comparison\nWe see that both approaches yield the same result, but what are the differences between them ? Apart from the syntax, we could check the performance of each of them. In order for us to do that, we are going to use the @time function, which returns the time and space used by a chosen function.\n@time begin\n    vec_1 = ones(length(x))\n    X = transpose([transpose(vec_1);transpose(x)])\n    (X\\y)\nend\n@time begin \n    model = lm(@formula(y ~ x), (;x, y))\n    GLM.coef(model)[1:2]\nend\n\n\n\nBuilt-in matrix approach :\n\n\n  0.000070 seconds (30 allocations: 40.266 KiB)\n\n\n2-element Vector{Float64}:\n 0.00995436086366171\n 0.4687272109439466\n\n\n\n\nGLM package approach :\n\n\n  0.000120 seconds (146 allocations: 20.438 KiB)\n\n\n2-element Vector{Float64}:\n 0.009954360863661988\n 0.46872721094394604\n\n\n\n\n\nIf we compare the information returned by the @time function, we see that the matrix computation approach seems to be slightly faster. However, we have to take into account that the lm() function does not only compute the coefficients, but alswo other informations, that are here not displayed, such as the confidence interval, and the probability associated to the t-statistic. In this sense, this comparison does not make justice to the GLM package. We can however remember that if we only wanted the coefficients of such a model, we could prefer the matrix multiplication approach if we need time."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html#multiple-linear-regression",
    "href": "posts/1_regressions_julia/index.html#multiple-linear-regression",
    "title": "Linear Regressions with Julia",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nHere, let us try to explain one continuous variable by multiple continuous variables.\nAgain, we first have to generate and plot the data :\n\n# Generating random data :\nx,y,w,z = (rand(100) for _ in 1:4)\n\n# Plotting the data :\nplot(scatter(x, y, z,\n    title = \"Static 3D plot\", \n    marker_z = w, label=\"data\"))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nRotatable plot\nNow, we could continue with the plot function of the default gr backend package of Plots, but to get rotable 3D plots, we can use the PlotlyJS package. Since Plot.jl integrates plotly, we could try just adding plotly() :\n\n# We switch from the gr() to the plotlyjs() backend within Plots :\nplotlyjs()\n# We plot :\nplot(scatter(x, y, z;\n        marker_z = w,\n        markersize = 2, \n        title = \"Rotatable 3D plot\",\n        label = \"data\", \n        xlabel = \"x\",\n        ylabel = \"y\", \n        zlabel = \"z\"))\n\n    \n    \n\n\nIt is now time for some model. Let us consider that the variable z can be explained linearly by the variables x and y, and that w does not provide supplementary information.\nWe are going to use the two methods we saw previously :\n# Using matrix computations :\n@time begin\n    X = [ones(length(x)) x y]\n    X\\z\nend\n# Using GLM : \n@time begin \n    model = lm(@formula(z ~ x + y), (;x, y, z))\n    coefs = GLM.coef(model)\n    coefs\nend\n\n\n\nBuilt-in matrix approach :\n\n\n  0.000045 seconds (33 allocations: 42.531 KiB)\n\n\n3-element Vector{Float64}:\n  0.36233333368806564\n -0.0017472521580554313\n  0.21145297793195228\n\n\n\n\nGLM package approach :\n\n\n  0.000265 seconds (205 allocations: 29.141 KiB)\n\n\n3-element Vector{Float64}:\n  0.362333333688066\n -0.0017472521580556221\n  0.2114529779319519\n\n\n\n\n\n\nAs before, both results are approximatively the same, with a small advantage for the matrix computation approach in performance terms.\n\n\n\n\n\nPlotting a multinomial regression\nNow let’s try to plot the plane of this regression :\n\nlinear_model(x,y) = coefs[1] .+ coefs[2] * x .+ coefs[3] * y\nplot(x,y,linear_model,\n    st=:surface,\n    title = \"Plane of a two dimensional linear regression\",\n    label=\"model\",\n    xlabel =\"x\",\n    ylabel = \"y\",\n    zlabel = \"z\")\n\n    \n    \n\n\nWhile I did not manage to plot simulatneously the points and the surface on the plot, it seemed a satisfactory result for the moment."
  },
  {
    "objectID": "posts/2_regressions_python/index.html",
    "href": "posts/2_regressions_python/index.html",
    "title": "Linear Regressions with Python",
    "section": "",
    "text": "In this article, I will try to do linear regressions with Python. This is also an occasion for me to try Python with Quarto."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#running-julia-and-r-in-the-same-document",
    "href": "posts/2_regressions_python/index.html#running-julia-and-r-in-the-same-document",
    "title": "Linear Regressions with Python",
    "section": "1.1. Running Julia and R in the same document",
    "text": "1.1. Running Julia and R in the same document\nDepending on the engine we are using, we can also make use of other languages in the same Quarto document. Specifying the python3 engine in the YAML header of our Quarto document, we can run Julia and R in it. For that, we need packages, such as JuliaCall for using Julia with R.\nTesting Julia yields :\n\n# Getting the version of Julia we are using : \nusing InteractiveUtils\nInteractiveUtils.versioninfo()\n\nJulia Version 1.10.3\nCommit 0b4590a550 (2024-04-30 10:59 UTC)\nBuild Info:\n  Built by Homebrew (v1.10.3)\n\n    Note: This is an unofficial build, please report bugs to the project\n    responsible for this build and not to the Julia project unless you can\n    reproduce the issue using official builds available at https://julialang.org/downloads\n\nPlatform Info:\n  OS: macOS (arm64-apple-darwin23.4.0)\n  CPU: 8 × Apple M1\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\nThreads: 1 default, 0 interactive, 1 GC (on 4 virtual cores)\nEnvironment:\n  DYLD_FALLBACK_LIBRARY_PATH = /Library/Frameworks/R.framework/Resources/lib:/Library/Java/JavaVirtualMachines/jdk-11.0.18+10/Contents/Home/lib/server\n\n\nTesting R yields :\n\n# Getting the version of R we are using : \nversion\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          4.0                         \nyear           2024                        \nmonth          04                          \nday            24                          \nsvn rev        86474                       \nlanguage       R                           \nversion.string R version 4.4.0 (2024-04-24)\nnickname       Puppy Cup                   \n\n\nSince the rest of the article will exclusively be using Python, we will not mention further details of engines in Quarto. It is however interesting to keep in mind that Quarto allows this multiple integration."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#generating-data",
    "href": "posts/2_regressions_python/index.html#generating-data",
    "title": "Linear Regressions with Python",
    "section": "2.1. Generating data",
    "text": "2.1. Generating data\nI found two main libraries to generate random data in Python. The first one is the random library, and the second one is the numpy library.\nWith the random library, we can use the random method to generate by default a float number between 0 and 1. A way to create a vector of 100 random values is to use the random method between square brackets, followed by the for instruction, similar to the one in Julia. This will generate a list of size fixed, such that :\n\nimport random\ny = ([random.random() for _ in range(100)])\nx = ([random.random()*y[_] for _ in range(100)])\n\nLet us comment a bit this code :\n\nIt is possible to call methods with the name of their libraries in front of it. In this sense, random.random() refers to the method random() inside the random library.\nThe underscore syntax _ combined with for loops to iterated for a set number of times. By default, the _ symbol just refers to the last variable in memory of Python, but it can also be used like that.\nThe range() function allows to create a range object in Python. More specifically :\n\n\n[It] return[s] an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. Range(i, j) produces i, i+1, i+2, …, j-1. Start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement).\n\nA second way of generating random data is using the numpy library. For that, we use the numpy.random.rand() method :\n\nimport numpy\nnumpy.random.rand(100)\n\narray([0.00592745, 0.75294248, 0.94689826, 0.47642028, 0.11962005,\n       0.52470106, 0.60702635, 0.15253373, 0.53457452, 0.38565149,\n       0.64664118, 0.52782941, 0.1863171 , 0.62393744, 0.47887005,\n       0.51752405, 0.6082846 , 0.94538496, 0.15588617, 0.93843073,\n       0.71304326, 0.91281546, 0.15744652, 0.65390069, 0.90430497,\n       0.02007971, 0.90876743, 0.7371541 , 0.13214819, 0.77117829,\n       0.41983029, 0.42378729, 0.68161776, 0.77530917, 0.10258949,\n       0.64014087, 0.92515723, 0.81516093, 0.32943734, 0.88071784,\n       0.45455588, 0.66861511, 0.82865439, 0.56225488, 0.9957281 ,\n       0.52904564, 0.47166687, 0.88949489, 0.45428736, 0.97020451,\n       0.99314305, 0.67423413, 0.56184371, 0.41641916, 0.54561917,\n       0.93896844, 0.09701167, 0.62007889, 0.87678115, 0.39321745,\n       0.9872945 , 0.97044126, 0.54605077, 0.51073665, 0.00533591,\n       0.97947303, 0.23759523, 0.22853751, 0.13700576, 0.9933836 ,\n       0.82715814, 0.56069471, 0.58494474, 0.93326851, 0.97248153,\n       0.90951482, 0.02044552, 0.01536296, 0.09132653, 0.31115691,\n       0.3089081 , 0.24117981, 0.42406074, 0.77607162, 0.07922152,\n       0.43397369, 0.36773676, 0.91578712, 0.4391219 , 0.46481153,\n       0.84583504, 0.11902362, 0.97803419, 0.98239256, 0.39096067,\n       0.39562847, 0.69702822, 0.84916839, 0.85820585, 0.07123363])\n\n\nHere, the numpy.random.rand() method also generates a random real number between 0 and 1. We are going to stick with the first x variable created for the rest of the section."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-the-data",
    "href": "posts/2_regressions_python/index.html#plotting-the-data",
    "title": "Linear Regressions with Python",
    "section": "2.2. Plotting the data",
    "text": "2.2. Plotting the data\nNow that we generated some data, let us plot it. To do that, we can use the matplotlib library. More specifically, we are going to use pyplot inside matplotlib, and are thus going to import matplotlib.pyplot :\n\nimport matplotlib.pyplot\nmatplotlib.pyplot.scatter(x,y)\n\n\n\n\n\n\n\n\n\nThe scatter() method of matplotlib.pyplot allows to plot a scatterplot with two vectors of the same size.\n\n\nThe scatter() “method” can also be considered as a function. This distinction seems to also exist in other programming languages and is definitely worth understanding. For the sake of linear regressions, it does not matter for now to develop a full understanding of this distinction. In the rest of this article, we will thus stick to the “method” term without further explanation."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#fitting-our-model",
    "href": "posts/2_regressions_python/index.html#fitting-our-model",
    "title": "Linear Regressions with Python",
    "section": "2.3. Fitting our model",
    "text": "2.3. Fitting our model\nWe are now going to fit our statistical model with our randomly generated values. In order for us to do that, we can use the scipy.stats library. This library has a linregress() method that allows to compute the OLS estimates to explain the second variable with the first one :\n\nimport scipy\nmodel = scipy.stats.linregress(x,y)\nmodel\n\nLinregressResult(slope=np.float64(0.8383718326044476), intercept=np.float64(0.30883317915377173), rvalue=np.float64(0.6159824384646858), pvalue=np.float64(9.024229215374189e-12), stderr=np.float64(0.10830516572266864), intercept_stderr=np.float64(0.03237551409033611))\n\n\nWe see that the scipy.stats.linregress() method returns a five elements object. To make this method easier to use, we could generate five variables, define a function with those variables and plot a line based on this function :\n\n# We define the five variables from the linregress() method :\nslope, intercept, r, p, std_err = scipy.stats.linregress(x, y)\n\n# We define a function returning a linear function with the slope and the intercept :\ndef myfunc(x):\n  return slope * x + intercept\n\n# We create a list object with the list() function that takes as an argument a map object, based on our function and on the variable x :\nmymodel = list(map(myfunc, x))\nmymodel\n\n[np.float64(0.3466552962850944), np.float64(0.4855711228389237), np.float64(0.9147031170175356), np.float64(0.5153160299254063), np.float64(0.3801609990790563), np.float64(0.6044597742873725), np.float64(0.8592707674606043), np.float64(0.36509600545338833), np.float64(0.5527195718240848), np.float64(0.5240474317193224), np.float64(0.6014795199237659), np.float64(0.42070431481441095), np.float64(0.3281109864632415), np.float64(0.609338479700958), np.float64(0.5272774936883462), np.float64(0.49793336361061025), np.float64(0.6615576168832237), np.float64(0.48558369820808656), np.float64(0.682856174322561), np.float64(0.7340434195846064), np.float64(0.5627113232349656), np.float64(0.32278236816126854), np.float64(0.5929799171046213), np.float64(0.35278841098901403), np.float64(0.3580677536604607), np.float64(0.42191435405309263), np.float64(0.3238418122945764), np.float64(0.3191886742819142), np.float64(0.3568292775067544), np.float64(0.32757438236782), np.float64(0.32671680069225056), np.float64(0.42326584087775343), np.float64(0.3301253898513797), np.float64(0.49109681714494313), np.float64(0.44458311312708415), np.float64(0.5199585587348523), np.float64(0.9186022591677041), np.float64(0.39114282592853183), np.float64(0.4014183383452378), np.float64(0.4877972869989182), np.float64(0.3266773853988873), np.float64(0.4588845589667885), np.float64(0.5815321192975246), np.float64(0.5075743754991987), np.float64(0.46377760017945563), np.float64(0.3574396105070033), np.float64(0.39249417206726944), np.float64(0.488752927533466), np.float64(0.35569069788803687), np.float64(0.49822895229055497), np.float64(0.5161923540876256), np.float64(0.5088533951852636), np.float64(0.769034375640042), np.float64(0.6326334342149869), np.float64(1.0423321590360448), np.float64(0.3296848512011278), np.float64(0.3707922844586858), np.float64(0.4266797387634848), np.float64(0.33652341146276105), np.float64(0.39743971330310107), np.float64(0.3336311390693917), np.float64(0.5564567995066856), np.float64(0.376091444906386), np.float64(0.45090231896434735), np.float64(0.5040838302265498), np.float64(0.4051622523445217), np.float64(0.3226383239044324), np.float64(0.33191401608324494), np.float64(0.4666930634968537), np.float64(0.570707699127708), np.float64(0.5617582353940804), np.float64(0.3124068143973404), np.float64(0.5292769029282633), np.float64(0.3203018487632795), np.float64(0.7373582721485279), np.float64(0.36890927811192537), np.float64(0.5325929851471497), np.float64(0.6323717424690725), np.float64(0.932503917572861), np.float64(0.33708525383560667), np.float64(0.6526942091151531), np.float64(0.35715817364214664), np.float64(1.015109148788574), np.float64(0.32887962550451877), np.float64(0.7060221022682294), np.float64(0.339489680159918), np.float64(0.6722012746136157), np.float64(0.7056671726580602), np.float64(0.745471258289146), np.float64(0.4308133963682357), np.float64(0.33121240452941486), np.float64(0.37579615641214226), np.float64(0.40511754269860345), np.float64(0.40202405377269135), np.float64(0.7391566857584171), np.float64(0.5956863712448213), np.float64(0.4803225716002546), np.float64(0.3907379210536772), np.float64(0.4214984627624902), np.float64(0.3109604583787798)]\n\n\nThe definition of the variable mymodel is worth explaning a bit more in details. The list() function is used to create a list object (which is considered one of the most versatile objects in Python), like the c() function creates a vector in R. The map() function returns a map object, defined as :\n\nmap(func, *iterables) –&gt; map object Make an iterator that computes the function using arguments from each of the iterables. Stops when the shortest iterable is exhausted.\n\nin the Python documentation.\nNow, if we want to add this line to our plot, we can run :\n\nmatplotlib.pyplot.scatter(x,y)\nmatplotlib.pyplot.plot(x, mymodel)\n\n\n\n\n\n\n\n\nWhile it is curious that the variable mymodel is displayed as a continuous line, it works, so I am fine with this solution for now.\nIf we want only the more relevant information, we can choose to only work with the two first elements of the array, and plot it, we can do in a more concise way :\n\ndef f(x):\n    return scipy.stats.linregress(x, y)[1]+numpy.dot(x,scipy.stats.linregress(x, y)[0])\n\nfloat(scipy.stats.linregress(x, y)[1]),float(scipy.stats.linregress(x, y)[0])\n\n(0.30883317915377173, 0.8383718326044476)\n\nmatplotlib.pyplot.scatter(x,y)\nmatplotlib.pyplot.plot(x, f(x))\n\n\n\n\n\n\n\n\nWe note here that we are using the numpy.dot() method from the numpy library. This method allows us to do the dot product of two different matrixes. In our case, doing only x * scipy.stats.linregress(x,y)[0] would have returned an error, since x and the other term are not of the same dimension. This method thus allows to avoid this error by vectorizing the multiplication.\nNow that we covered a how to perform a simple linear regression with Python, let us move to a multiple linear regression."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#running-a-multiple-linear-regression",
    "href": "posts/2_regressions_python/index.html#running-a-multiple-linear-regression",
    "title": "Linear Regressions with Python",
    "section": "3.1. Running a multiple linear regression",
    "text": "3.1. Running a multiple linear regression\nFirst, let us generate some random data and fit our model.\nTo generate random data, we can do :\n\n# We randomly generate four variables of length 100 :\nw,x,y,z = (numpy.dot([random.random() for _ in range(100)],_) for _ in (1,2,3,4))\n# Just to check the length : \nlen(y)\n\n100\n\n\nTo run a multinomial linear regression that satisfies the OLS equation, we could use several libraries. Let us say that we want to explain the z variable by x and y.\n\n3.1.1. The sklearn library\nOne approach that seems to be widely used is to use the sklearn library. More specifically, we could use the sklearn.linear_model package :\n\nimport sklearn.linear_model\n\n# We create an object that regroups x, y and w :\nregressors = numpy.column_stack((x,y,w))\n\n# We create the object 'model' that has the LinearRegression() class from the sklearn.linear_model package : \nmodel = sklearn.linear_model.LinearRegression()\n \n# Fit the model to the data\nmodel.fit(regressors, z)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nmodel.coef_\n\narray([0.02372518, 0.22686716, 0.08769466])\n\nmodel.intercept_\n\nnp.float64(1.5537197507494207)\n\n\nThe model.intercept_ value is the intercept of z, and the coefficients are consistent with the regressors object. The first one corresponds to the effect of x, the second to the effect of y, and the last one to the effect of w.\n\n\n3.1.2. The statsmodels library\nTo get a nicer display and final object, we can alternatively use the statsmodels library.\n\nimport statsmodels.api\n\n# Create the object and specify an intercept :\nregressors = statsmodels.api.add_constant(regressors)\nmodel = statsmodels.api.OLS(z,regressors)\n\n# Run the model :\nresults = model.fit()\n\n# Print the regression table : \nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.030\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.9966\nDate:                Fri, 05 Jul 2024   Prob (F-statistic):              0.398\nTime:                        14:07:22   Log-Likelihood:                -152.78\nNo. Observations:                 100   AIC:                             313.6\nDf Residuals:                      96   BIC:                             324.0\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.5537      0.358      4.342      0.000       0.843       2.264\nx1             0.0237      0.202      0.118      0.907      -0.377       0.424\nx2             0.2269      0.135      1.684      0.095      -0.041       0.494\nx3             0.0877      0.400      0.219      0.827      -0.706       0.881\n==============================================================================\nOmnibus:                       17.960   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                5.139\nSkew:                           0.179   Prob(JB):                       0.0766\nKurtosis:                       1.949   Cond. No.                         9.43\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can also access to the itnercept and the coefficients directly, by doing :\n\nresults.params\n\narray([1.55371975, 0.02372518, 0.22686716, 0.08769466])"
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-a-static-3d-plot",
    "href": "posts/2_regressions_python/index.html#plotting-a-static-3d-plot",
    "title": "Linear Regressions with Python",
    "section": "3.2. Plotting a static 3D plot",
    "text": "3.2. Plotting a static 3D plot\nIf we now want to limit ourselves to a four dimensional problem, we can plot 3D graphs with colors representing the fourth dimension. In this section, we are first going to plot a static graph.\n\n# We first create the framework of a plot, that we name \"figure\" : \nfigure = matplotlib.pyplot.figure()\n\n# In this figure, we are going to use the add_subplot method to create a three dimensional projection : \nprojection = figure.add_subplot(projection='3d')\n\n# In this three dimensional space, we are going to project our points in a scatterplot way.\n# The argument 'c' stands for the color of the points :\nprojection.scatter(x,y,z, c = w)\n\n# Finally, we give each of our axis a name : \nprojection.set_xlabel(\"X\")\nprojection.set_ylabel(\"Y\")\nprojection.set_zlabel(\"Z\")\n\n\n\n\n\n\n\n\nThe syntax of this example can be a bit hard to understand at first, but is required when using matplotlib.pyplot library."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-a-rotatable-3d-plot",
    "href": "posts/2_regressions_python/index.html#plotting-a-rotatable-3d-plot",
    "title": "Linear Regressions with Python",
    "section": "3.3. Plotting a rotatable 3D plot",
    "text": "3.3. Plotting a rotatable 3D plot\nNow it’s time for a rotatable plot, like in the previous article. Fortunately, the plotly library does also exist for Python, which is great, because I did not find any other library that yielded a similar result. However, to use plotly, we need the pandas library.\nWe can thus create a dataframe object from a dictionary with the four previously created vectors. If we only want to display a 3D scatter plot, we can run :\n\nimport plotly.express\nimport pandas\n\n# We create a four columns dataframe with our four previously generated vectors : \ndata = {'c1':[x],'c2':[y],'c3':[z],'c4':[w]}\nX = pandas.DataFrame(data)\n\nfig = plotly.express.scatter_3d(X, x,y,z, color=w)\nfig.show()\n\n                        \n                                            \n\n\nWe note that this yields a plot with similar features to the ones obtained in the previous article with Julia. The reason for that is that both obtained plots are done using the plotly library.\nFor the rest of the section, we are going to limit ourselves to only two regressands, excluding the w variable, since the display will be easier to understand this way :\n\nX = numpy.column_stack((x,y))\nX = statsmodels.api.add_constant(X)\nmodel = statsmodels.api.OLS(z,X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.030\nModel:                            OLS   Adj. R-squared:                  0.010\nMethod:                 Least Squares   F-statistic:                     1.485\nDate:                Fri, 05 Jul 2024   Prob (F-statistic):              0.232\nTime:                        14:07:23   Log-Likelihood:                -152.81\nNo. Observations:                 100   AIC:                             311.6\nDf Residuals:                      97   BIC:                             319.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.5915      0.312      5.099      0.000       0.972       2.211\nx1             0.0273      0.200      0.136      0.892      -0.370       0.425\nx2             0.2292      0.134      1.715      0.090      -0.036       0.494\n==============================================================================\nOmnibus:                       18.620   Durbin-Watson:                   2.015\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                5.252\nSkew:                           0.186   Prob(JB):                       0.0724\nKurtosis:                       1.941   Cond. No.                         6.97\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNow, we need more lines to plot a surface. More specifically, we are going to use plotly.graph_objects and make use of the meshgrid method :\n\nimport plotly.graph_objects\n\n# We first store the information of our regression : \nintercept, coef1, coef2 = results.params\n\n# We then create meshgrids for x,y, and z : \nx_range = numpy.linspace(x.min(), x.max(), 10)\ny_range = numpy.linspace(y.min(), y.max(), 10)\nx_grid, y_grid = numpy.meshgrid(x_range, y_range)\nz_grid = intercept + coef1 * x_grid + coef2 * y_grid\n\n# We now generate a surface object with the grids we just created\nsurface = plotly.graph_objects.Surface(\n    x=x_grid, y=y_grid, z=z_grid,\n    colorscale='Viridis', opacity=0.5\n)\n\n# We create a scatter object, to be able to plot it with the surface :\nscatter = plotly.graph_objects.Scatter3d(\n    x=x, y=y, z=z,\n    mode='markers',\n    marker=dict(size=5, color=w, colorscale='Viridis')\n)\n\n# We create a figure object, with the scatter and surface tha we just generated : \nfig = plotly.graph_objects.Figure(data = [scatter, surface])\n\n# We display the figure : \nfig.show()\n\n                        \n                                            \n\n\nThis looks satisfactory. I even manage to plot the points with the surface of the regression in the 3D plot."
  }
]