[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "Literature Review of natural hazards damages (in Progress)\n\n\n\n\n\n\neconomics\n\n\nenvironment\n\n\nnatural hazard\n\n\nenvironmental damage\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regressions with Python\n\n\n\n\n\n\npython\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regressions with Julia\n\n\n\n\n\n\njulia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose statement\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nPaulo Gugelmo Cavalheiro Dias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html",
    "href": "posts/3_presentation_macro_3/index.html",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "",
    "text": "Warning\n\n\n\nThis is a draft for our Macro3 presentation. It is currently under construction.\nThis article aims to summarize in a detailed fashion the following article for a Macroeconomics 3 presentation."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#wealth-inequality",
    "href": "posts/3_presentation_macro_3/index.html#wealth-inequality",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Wealth inequality",
    "text": "Wealth inequality\n6 -\nThey underly the fact that U.S. wealth inequalities have exploded recently, and define three ways of wealth accumulation : 1. Wages in the short term. 2. Investment (linked to RoI) in the long term. 3. Intergenerational transfers at the life term. They point out that wealth has major effect on several other variables, such as (1) education performances, education inequalities, (2) physical health, and (3) emotional well-being. They explain that income and wages have strongly decreased for middle and low wage workes.\n7 - About Investment and Long term factors of wealth accumulaition, the authors point out the role of the real estate markets in the increase of inequalities.\n8 - Finally, the differences of interest rates on loans have strongly risen, and play a big role in wealth inequality, especially with the 2008 crisis that affected poor people more strongly.\n9 - Globally, wealth inequality has strongly icnreased at the extremities of the wealth distribution, and policies have also participated to that."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#natural-hazard-and-wealth-inequality",
    "href": "posts/3_presentation_macro_3/index.html#natural-hazard-and-wealth-inequality",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Natural Hazard and Wealth Inequality",
    "text": "Natural Hazard and Wealth Inequality\n10 - In this paper, they put the focus on natural hazard with physical damage on property and assets, which can be public or private. Historically, they identify the year of 1755, with the earthbreak in Lisboa, as the begin of the development of an answer from the State.\n11 - In the US, the insurance against natural hazard from the State begins in the early 1800s, with 1803 tariffs to help victimes of fire damages. In 1811 and 1812, lands are freely given to victims of displacement, and in the 1830s, there exists a de facto policy of federal disaster assistance. In 1950, there is the Disaster Relief Act (DRA), that is the main milestone for the n.h. protection policy in the US. It allows for : 1) Immediate relief (though housing notably) 2) hazard insurance programs 3) rebuilding damaged infrastructure 4) low-interest loans.\n12 - Under other institutions, the authors identify the Federal Emergency Management Agency (FEMA), that is responsible for redistribution of money in n.h. cases, and the National Flood Insurance Program (NFIP), the biggest hazard insurer of the US. Also, there is the private sector of insurance.\n13 - The author underline the fact that private and public assistance are focused on restoration of property and wealth, which implies differences in recovery rate and quality in function of the initial wealth of the insured people. Notably, the mention that research has found that poor and non-property owners people are more exposed to risks such as : 1) job loss 2) Having to move (displacement) 3) Having to pay a higher rent, 4) Experimenting a decrease in savings.\n14 - They mention the fact that in some natural hazards cases, there is often a reduction or a suspension in labor quality protection policies, and they give the example of regulation suspension after Katrina."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#research-design",
    "href": "posts/3_presentation_macro_3/index.html#research-design",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Research Design",
    "text": "Research Design\n15 - The authors then present their research design. They use panel data, to study the effect of NH damage on welath distribution. This effect differs in function of considered population, and vary especially strongly when race, education level, and ownership are considered.\n16 - The paper also inquires about two main hypothesis : 1) PRivate insurance does not totally prevent increase in wealth inequality from NH damages. 2) the FEMA aid (in net terms) differs in population and contributes to wealth inequality."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#data",
    "href": "posts/3_presentation_macro_3/index.html#data",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Data",
    "text": "Data\n17 - The authors now present the used data. THey begin with the Panel Study of Income Dynamics (PSID), a panel dataset that track US families from a “representative sample” from 1968 to now, with information on county and neighbourhood information.\n18 - They also use the Spatial Hazard Events and Losses Database (SHELD), that is maintained by the Hazards and Vulnerability Research Institute (HVRI) since 1960 until now. In this dataset, 18 types of natural hazards and their link with (1) fatalities and (2) property damages, are taken into account.\n19 - They also make use of the Devennial Census Long form for contextualization.\n20 - They finally use the FEMA Projects Summary."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#measurement",
    "href": "posts/3_presentation_macro_3/index.html#measurement",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Measurement",
    "text": "Measurement\n\nWealth Trajectories\n21 - The define Wealth as in the PSID dataset, where it is the sum, at the close family level*, of = - Saving accounts, - Checking accounts, - Real estate holdings, - Equity, - Vehicles, - Farms, - Businesses, - Stocks, - Annuities / IRAs, - Minus all reported debts.\nThe distribution they get of the wealth is right-skewed, i.e. inflated by the very rich. To work with the data, they : 1) added a “global minimum” to all raw data, to avoid negative values, and the used the squared root to compress the different wealth levels, 2) they transformed the data to get a normal distribution.\n\n\n\nTABLE 1.\n\n\n\n\nNatural Hazard Damages\n22 - In the SHELDUS dataset, the natural hazards damages correspond to the “direct damage to (non-crop) property” variable, in dollars. They standardize it to the 2012 U.S. dollar value, and apply the natural log to get the levels of damages.\n\n\nOther Key Variables\n23 - The authors underline the fact that some other key variables have to be taken into account. They identify three main dimensions of social stratification : 1) race (4 mutually exclusive categories : white, black, latino (or hispanic), and “other”), 2) education (in number of total years of school completed, and 3) homeowner status (1 or 0).\n24 - To get the private insurance investment, they compute the sum of all premium paid for home and car coverage during the interview year. They do not have more data, although it would be better.\n25 - To get the Federal Assistance variable, they sum the non fire related FEMA aid. Here the authors detail a bit the aid asking process. First, when a natural hazard is too big to be handled by the county or the state, they ask the federal authorities. If the President declare the existence of a major disaster, the FEMA then takes action and does money transfers and other kinds of help, such as : financial help, temporary housing, house voucher, uninsured personal needs, and even direct replacement of private property sometimes.\n\n\nControl Variables\nThey also include several control variables, at three main levels : the individual, the neighhorhood, and the population.\n26 - The individual level variables are : age, foreign birth, marital status, and number of children under 18 years old who are living within the household, and if the individuals have moved in the last two years.\n27 - The variables at the neighborhood level are : the median income of the neighborhood, the number of adults with at least a bachelor degree, and the number of adults currently employed.\n28 - Finally, the variables at the population level are the total population, and the ubran-rural variable from 1 to 9."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#sampling-and-modeling",
    "href": "posts/3_presentation_macro_3/index.html#sampling-and-modeling",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Sampling and Modeling",
    "text": "Sampling and Modeling\n29 - Historically, the choice of following the “head of the household” led to only men in the samples, which biased the data from the difference in marital status.\n30 - THe author did run one model per gender in the household, but due to very similar results, they only present the female model with at least four over seven interviews.\n31-32 - Econometrics things."
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#results",
    "href": "posts/3_presentation_macro_3/index.html#results",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Results",
    "text": "Results\n32 - The authors first mention the difference in aid against natural hazard damage between 1991-2001 and 2005-2007, showing a clear increase from $237 millions to $1,15 billion. Also, they mention that some counties are especially touched by the phenomenon, such as the Linn County, in Iowa.\n\nNatural Hazard Damages and Wealth Inequality\n33 - They then refer to Table 2. This table presents several models that try to explain wealth by multiple variables, depending on the model. They first talk about the first model, that only takes the level of hazard damage and the year of the regression. In the second model, they include all the control variables mentioned previously, and they see clearly that the race variables have a negative impact on wealth.\n \n34 - In the third model, they also include interaction between hazard and race variables. The effects of those interaction effects are strong and significant. The way to interpret them could be : for a Black person, being affected by a one dollar hazard damage explains a wealth decrease of 12.62 dollars in wealth accumulation from one period to the next one. We can see the effect of the race variable in the Panel A.\n35 - The authors then test for the effect of education in the next model. The panel B is a projection of the model that takes into account the education variable. In the y-axis, there is “Wealth in 2013”. We can see the three panels of Figure 1 as : for a certain level of natural hazard damage, how much wealthy is a certain subgroup of the population ? We see clearly that less educated people are poorer and this wealth inequality is increasing with the amount of natural damage the county went through.\n36 - In the Model 5, they take into consideration the homeownership. They find that homeowners are richer in zones that are more exposed to natural hazards.\n\n\n\nFigure 1\n\n\n37 - Testing all the variables, they find that the ones that cause the biggest wealth gap are race, education, and homeownership (REC). The way to think about it, is that natural hazards can affect people very differently depending on those three variables. In the FIgure 2, they compare the wealth trajectory of an educated white owner and a 10-th grade black renter.\n\n\n\nFigure 2\n\n\n38 - Finally, in the last model, model 7, they test for privae insurance. First, they find that more private insurance predicts a bigger wealth accumulation, and seems therefore to confirm that it does bring larger payouts due to a better coverage. Secondly, including private insurance does not reduce the effect of the other three main factors (race, education, homeownership). In this sense, private insurance does not totally prevent wealth inequality increase due to natural hazards.\n\n\nFEMA Aid and Wealth Inequality\n39 - They now want to test the FEMA aid : Does the FEMA aid differ in function of wealth ? In Table 3, they try to explain the FEMA aid by hazard damge, controlling for total population, rurality scale and the year. They do find that the effect exists, but is very weak, and not very robust. Therefore, they rerun the models (only five this time), but this time including the FEMA aid and private insurance premium.\n\n\n\nTable 3\n\n\n40 - The main conclusion one can draw from these models is that FEMA aid and wealth inequalities are positively linked. Also, Black and Latino people accumulate less when they live in counties that received a lot of FEMA aid. The inverse is true for White people. The effect can be seen in the Interaction Terms of the Model 1. The way to interpret them, is again “being black and living in a county receiving FEMA aid” does affect in [coefficient] your wealth accumulation from one period to another. In the second model, they take out race interaction terms, but take into account interaction term between FEMA and Education. In the third model, they include homeownership.\n \n41 - They also run the models in with interaction effects between the FEMA and othe control variables. They find that “the more wealth one has, the more one benefits from living in a county that receives more FEMA aid”.\n42 - The authors advance that the FEMA aid “polarize wealth along racial, educational, and initial wealth lines, as ecidenced by the statistically significant interaction coefficient”. Then, in Figure 3, they reproduce the Figure 2, but with FEMA Aid in the x-axis. The divergence seems even bigger.\n\n\n\nFigure 3"
  },
  {
    "objectID": "posts/3_presentation_macro_3/index.html#conclusion",
    "href": "posts/3_presentation_macro_3/index.html#conclusion",
    "title": "Literature Review of natural hazards damages (in Progress)",
    "section": "Conclusion",
    "text": "Conclusion\n43 - Wealth inequalities are explained by different elements, and this paper aims to show that natural hazards damages are one of them. Also, they undrly the importance of taking into account the interaction between natural hazards damages and pre-existing social dynamics, such as race, education, and ownership.\n44 - In that way, they advance that we should think more about the policies managing environmental hazards.\n45 - They recognize that the precise understanding on how natural hazards polarize wealth inequality is still incomplete. They thus insist on further research and taking the aid into consideration in the study of wealth inequalities : “equal aid is not equitable aid”.\n46 - They advocate for a more important taking into consideration of the interactions between social and environmental dynamics. For that, they advance that viewing the U.S. society as an inequal and heterogeneous group would be better in comparison to a monolitic group view. THey make a reference to Stalling (2002), and re-underly the fact that natural hazards are more detrimental to socially marginalized groups. They therefore advance that the FEMA help should mitigate this phenomenon, contrary to what is happening right now.\n47 - They conclude by saying that the FEMA is necessary, but that we just have to rethink how aid is brought."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html",
    "href": "posts/1_regressions_julia/index.html",
    "title": "Linear Regressions with Julia",
    "section": "",
    "text": "The first time I heard about Julia was a few months ago, in an introductory class to programming. Since then, I have come to know a bit more about the reputation of the language, supposely very efficient and especially suited for scientific computations.\nIn this article, I will try to perform some basic linear regressions with Julia and to plot them."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html#simple-linear-regression",
    "href": "posts/1_regressions_julia/index.html#simple-linear-regression",
    "title": "Linear Regressions with Julia",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nHere, let us try to explain one continuous variable by another continuous variable.\nFirst, those are the packages we are going to use :\n\nusing Plots\nusing GLM\n\n\nThe Plots package seems to be the most general package to plot data with Julia. It is a simple interface to several underlying plotting packages, like gr or plotly. In this sense, it allows to easily switch between different frameworks that have different features, while keeping the same syntax.\nThe GLM package seems to be the main package for Generalized Linear Models (GLM) with Julia. It will be useful in the computation of our simple and multinomial linear models.\n\nNow, let us generate some data :\n\nx = range(0, 1, length=100)\n# Generate data : \ny = rand(100) .* x\n\n# Check the data :\nsize(y)\n\n(100,)\n\n\n\nThe function range() in Julia allows to create an array with a starting and an ending value. Although having similar properties than vectors, arrays created with the range() function have the “Range” type (or StepRangeLen to be exact). Here, I just initialise the x array to compute an easy to use y vector.\nThe function rand() in Julia allows to randomly generate numbers. The default settings of the function make it so that the drawn data is of type Float64 between \\(0\\) and \\(1\\). Its argument, set to the value of \\(100\\), indicates the number of random values that are to be drawn. Therefore, rand(100) returns a vector of 100 Float64 numbers.\nThe .* function is a vectorized multiplication. In Julia, adding a dot . before a mathematical operator allows to apply this operation to all the elements of a vector. If we wanted to add \\(2\\) to all the elements of the y vector, we should have written y .+= 2.\nThe last function size() is useful to get more dimension of the array we consider. In our case, it yields (100,) because it is a \\(100\\) rows vector. We would get the same if we did size(y).\n\nNow, let us plot the data we just created :\n\nplot(\n    scatter(x,y, label = \"data\"),\n    title = \"Generating Random Data\",\n    ylabel = \"Random variable\",\n    xlabel = \"x\",\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function plot() in Julia comes from the Plots package. It is the default plotting function, with the gr backend by default (we will get to that later on). It contains several elements :\n\nThe function scatter() allows to plot a scatterplot, i.e. independent points that are not linked by a line. If we had not indicated the scatter() function, it would have plotted a line between all the points.\nThe label value is the one that gets displayed within the box. Specifying “data” allows to avoid the “y1” default value.\ntitle, ylabel and xlabel are pretty straightforward to understand.\n\nIt is now time to run a simple linear regression. Two simple ways to run such a model are :\n\nUsing the GLM package,\nUsing the built-in matrix computation function of Julia.\n\n\nGLM package\nFirst, with the GLM package :\n\nmodel = lm(@formula(y ~ x), (;y, x))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int32}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     t  Pr(&gt;|t|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.00995436   0.03272    0.30    0.7616  -0.0549775  0.0748862\nx            0.468727     0.0565302  8.29    &lt;1e-12   0.356545   0.58091\n──────────────────────────────────────────────────────────────────────────\n\n\nNow, if we want to plot our model with the data, we can run :\n\ncoefs = GLM.coef(model)\nplot!((x) -&gt; coefs[1] + coefs[2] * x, label=\"model\", \n    title = \"Simple linear regression\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to include the equation of our linear model in the legend of the plot, we can use the LaTeXStrings.jl package :\n\nusing LaTeXStrings\nplot(title = \"Simple linear regression\", \n    scatter(x,y, label = false),\n    (x) -&gt; coefs[1] + coefs[2] * x, label=latexstring(\"y=$(coefs[1]) + $(coefs[2]) * x\"))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we want it to be outside the plot :\n\nplot!(legend=:outerbottom, legendcolumns=1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix computation\nSecond, with the built-in matrix computation function :\n\nX = [ones(length(x)) x] \n(X\\y)\n\n2-element Vector{Float64}:\n 0.00995436086366171\n 0.4687272109439466\n\n\nThe first line creates a matrix \\(X\\) such that \\(X\\in\\mathbb{R}^{100\\times 100}\\), with its first columns being only ones, and its second column being the vector of \\(x\\).\nWithout entering into the details, it does exactly what the OLS method does, but in a more rudimentary way, using the normal equation to find the OLS estimates.\n\n\nPerformance comparison\nWe see that both approaches yield the same result, but what are the differences between them ? Apart from the syntax, we could check the performance of each of them. In order for us to do that, we are going to use the @time function, which returns the time and space used by a chosen function.\n@time begin\n    vec_1 = ones(length(x))\n    X = transpose([transpose(vec_1);transpose(x)])\n    (X\\y)\nend\n@time begin \n    model = lm(@formula(y ~ x), (;x, y))\n    GLM.coef(model)[1:2]\nend\n\n\n\nBuilt-in matrix approach :\n\n\n  0.000070 seconds (30 allocations: 40.266 KiB)\n\n\n2-element Vector{Float64}:\n 0.00995436086366171\n 0.4687272109439466\n\n\n\n\nGLM package approach :\n\n\n  0.000120 seconds (146 allocations: 20.438 KiB)\n\n\n2-element Vector{Float64}:\n 0.009954360863661988\n 0.46872721094394604\n\n\n\n\n\nIf we compare the information returned by the @time function, we see that the matrix computation approach seems to be slightly faster. However, we have to take into account that the lm() function does not only compute the coefficients, but alswo other informations, that are here not displayed, such as the confidence interval, and the probability associated to the t-statistic. In this sense, this comparison does not make justice to the GLM package. We can however remember that if we only wanted the coefficients of such a model, we could prefer the matrix multiplication approach if we need time."
  },
  {
    "objectID": "posts/1_regressions_julia/index.html#multiple-linear-regression",
    "href": "posts/1_regressions_julia/index.html#multiple-linear-regression",
    "title": "Linear Regressions with Julia",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nHere, let us try to explain one continuous variable by multiple continuous variables.\nAgain, we first have to generate and plot the data :\n\n# Generating random data :\nx,y,w,z = (rand(100) for _ in 1:4)\n\n# Plotting the data :\nplot(scatter(x, y, z,\n    title = \"Static 3D plot\", \n    marker_z = w, label=\"data\"))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nRotatable plot\nNow, we could continue with the plot function of the default gr backend package of Plots, but to get rotable 3D plots, we can use the PlotlyJS package. Since Plot.jl integrates plotly, we could try just adding plotly() :\n\n# We switch from the gr() to the plotlyjs() backend within Plots :\nplotlyjs()\n# We plot :\nplot(scatter(x, y, z;\n        marker_z = w,\n        markersize = 2, \n        title = \"Rotatable 3D plot\",\n        label = \"data\", \n        xlabel = \"x\",\n        ylabel = \"y\", \n        zlabel = \"z\"))\n\n    \n    \n\n\nIt is now time for some model. Let us consider that the variable z can be explained linearly by the variables x and y, and that w does not provide supplementary information.\nWe are going to use the two methods we saw previously :\n# Using matrix computations :\n@time begin\n    X = [ones(length(x)) x y]\n    X\\z\nend\n# Using GLM : \n@time begin \n    model = lm(@formula(z ~ x + y), (;x, y, z))\n    coefs = GLM.coef(model)\n    coefs\nend\n\n\n\nBuilt-in matrix approach :\n\n\n  0.000045 seconds (33 allocations: 42.531 KiB)\n\n\n3-element Vector{Float64}:\n  0.36233333368806564\n -0.0017472521580554313\n  0.21145297793195228\n\n\n\n\nGLM package approach :\n\n\n  0.000265 seconds (205 allocations: 29.141 KiB)\n\n\n3-element Vector{Float64}:\n  0.362333333688066\n -0.0017472521580556221\n  0.2114529779319519\n\n\n\n\n\n\nAs before, both results are approximatively the same, with a small advantage for the matrix computation approach in performance terms.\n\n\n\n\n\nPlotting a multinomial regression\nNow let’s try to plot the plane of this regression :\n\nlinear_model(x,y) = coefs[1] .+ coefs[2] * x .+ coefs[3] * y\nplot(x,y,linear_model,\n    st=:surface,\n    title = \"Plane of a two dimensional linear regression\",\n    label=\"model\",\n    xlabel =\"x\",\n    ylabel = \"y\",\n    zlabel = \"z\")\n\n    \n    \n\n\nWhile I did not manage to plot simulatneously the points and the surface on the plot, it seemed a satisfactory result for the moment."
  },
  {
    "objectID": "posts/purpose_statement/index.html",
    "href": "posts/purpose_statement/index.html",
    "title": "Purpose statement",
    "section": "",
    "text": "Hello, I am Paulo Gugelmo Cavalheiro Dias, currently a graduate level economics students, and this is the first post of my blog, in which I would like to write its purpose statement.\nI want this blog to be a way of sharing my progress in the topics I am interested in, i.e. economics and related subjects, such as programming for social sciences, environmental economics, etc. To do so, I will try to write regular reports on my work in social science, about my classes, readings and other materials."
  },
  {
    "objectID": "posts/2_regressions_python/index.html",
    "href": "posts/2_regressions_python/index.html",
    "title": "Linear Regressions with Python",
    "section": "",
    "text": "In this article, I will try to do linear regressions with Python. This is also an occasion for me to try Python with Quarto."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#running-julia-and-r-in-the-same-document",
    "href": "posts/2_regressions_python/index.html#running-julia-and-r-in-the-same-document",
    "title": "Linear Regressions with Python",
    "section": "1.1. Running Julia and R in the same document",
    "text": "1.1. Running Julia and R in the same document\nDepending on the engine we are using, we can also make use of other languages in the same Quarto document. Specifying the python3 engine in the YAML header of our Quarto document, we can run Julia and R in it. For that, we need packages, such as JuliaCall for using Julia with R.\nTesting Julia yields :\n\n# Getting the version of Julia we are using : \nusing InteractiveUtils\nInteractiveUtils.versioninfo()\n\nJulia Version 1.10.3\nCommit 0b4590a550 (2024-04-30 10:59 UTC)\nBuild Info:\n  Built by Homebrew (v1.10.3)\n\n    Note: This is an unofficial build, please report bugs to the project\n    responsible for this build and not to the Julia project unless you can\n    reproduce the issue using official builds available at https://julialang.org/downloads\n\nPlatform Info:\n  OS: macOS (arm64-apple-darwin23.4.0)\n  CPU: 8 × Apple M1\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\nThreads: 1 default, 0 interactive, 1 GC (on 4 virtual cores)\nEnvironment:\n  DYLD_FALLBACK_LIBRARY_PATH = /Library/Frameworks/R.framework/Resources/lib:/Library/Java/JavaVirtualMachines/jdk-11.0.18+10/Contents/Home/lib/server\n\n\nTesting R yields :\n\n# Getting the version of R we are using : \nversion\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          4.0                         \nyear           2024                        \nmonth          04                          \nday            24                          \nsvn rev        86474                       \nlanguage       R                           \nversion.string R version 4.4.0 (2024-04-24)\nnickname       Puppy Cup                   \n\n\nSince the rest of the article will exclusively be using Python, we will not mention further details of engines in Quarto. It is however interesting to keep in mind that Quarto allows this multiple integration."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#generating-data",
    "href": "posts/2_regressions_python/index.html#generating-data",
    "title": "Linear Regressions with Python",
    "section": "2.1. Generating data",
    "text": "2.1. Generating data\nI found two main libraries to generate random data in Python. The first one is the random library, and the second one is the numpy library.\nWith the random library, we can use the random method to generate by default a float number between 0 and 1. A way to create a vector of 100 random values is to use the random method between square brackets, followed by the for instruction, similar to the one in Julia. This will generate a list of size fixed, such that :\n\nimport random\ny = ([random.random() for _ in range(100)])\nx = ([random.random()*y[_] for _ in range(100)])\n\nLet us comment a bit this code :\n\nIt is possible to call methods with the name of their libraries in front of it. In this sense, random.random() refers to the method random() inside the random library.\nThe underscore syntax _ combined with for loops to iterated for a set number of times. By default, the _ symbol just refers to the last variable in memory of Python, but it can also be used like that.\nThe range() function allows to create a range object in Python. More specifically :\n\n\n[It] return[s] an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. Range(i, j) produces i, i+1, i+2, …, j-1. Start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement).\n\nA second way of generating random data is using the numpy library. For that, we use the numpy.random.rand() method :\n\nimport numpy\nnumpy.random.rand(100)\n\narray([0.00592745, 0.75294248, 0.94689826, 0.47642028, 0.11962005,\n       0.52470106, 0.60702635, 0.15253373, 0.53457452, 0.38565149,\n       0.64664118, 0.52782941, 0.1863171 , 0.62393744, 0.47887005,\n       0.51752405, 0.6082846 , 0.94538496, 0.15588617, 0.93843073,\n       0.71304326, 0.91281546, 0.15744652, 0.65390069, 0.90430497,\n       0.02007971, 0.90876743, 0.7371541 , 0.13214819, 0.77117829,\n       0.41983029, 0.42378729, 0.68161776, 0.77530917, 0.10258949,\n       0.64014087, 0.92515723, 0.81516093, 0.32943734, 0.88071784,\n       0.45455588, 0.66861511, 0.82865439, 0.56225488, 0.9957281 ,\n       0.52904564, 0.47166687, 0.88949489, 0.45428736, 0.97020451,\n       0.99314305, 0.67423413, 0.56184371, 0.41641916, 0.54561917,\n       0.93896844, 0.09701167, 0.62007889, 0.87678115, 0.39321745,\n       0.9872945 , 0.97044126, 0.54605077, 0.51073665, 0.00533591,\n       0.97947303, 0.23759523, 0.22853751, 0.13700576, 0.9933836 ,\n       0.82715814, 0.56069471, 0.58494474, 0.93326851, 0.97248153,\n       0.90951482, 0.02044552, 0.01536296, 0.09132653, 0.31115691,\n       0.3089081 , 0.24117981, 0.42406074, 0.77607162, 0.07922152,\n       0.43397369, 0.36773676, 0.91578712, 0.4391219 , 0.46481153,\n       0.84583504, 0.11902362, 0.97803419, 0.98239256, 0.39096067,\n       0.39562847, 0.69702822, 0.84916839, 0.85820585, 0.07123363])\n\n\nHere, the numpy.random.rand() method also generates a random real number between 0 and 1. We are going to stick with the first x variable created for the rest of the section."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-the-data",
    "href": "posts/2_regressions_python/index.html#plotting-the-data",
    "title": "Linear Regressions with Python",
    "section": "2.2. Plotting the data",
    "text": "2.2. Plotting the data\nNow that we generated some data, let us plot it. To do that, we can use the matplotlib library. More specifically, we are going to use pyplot inside matplotlib, and are thus going to import matplotlib.pyplot :\n\nimport matplotlib.pyplot\nmatplotlib.pyplot.scatter(x,y)\n\n\n\n\n\n\n\n\n\nThe scatter() method of matplotlib.pyplot allows to plot a scatterplot with two vectors of the same size.\n\n\nThe scatter() “method” can also be considered as a function. This distinction seems to also exist in other programming languages and is definitely worth understanding. For the sake of linear regressions, it does not matter for now to develop a full understanding of this distinction. In the rest of this article, we will thus stick to the “method” term without further explanation."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#fitting-our-model",
    "href": "posts/2_regressions_python/index.html#fitting-our-model",
    "title": "Linear Regressions with Python",
    "section": "2.3. Fitting our model",
    "text": "2.3. Fitting our model\nWe are now going to fit our statistical model with our randomly generated values. In order for us to do that, we can use the scipy.stats library. This library has a linregress() method that allows to compute the OLS estimates to explain the second variable with the first one :\n\nimport scipy\nmodel = scipy.stats.linregress(x,y)\nmodel\n\nLinregressResult(slope=np.float64(0.8383718326044476), intercept=np.float64(0.30883317915377173), rvalue=np.float64(0.6159824384646858), pvalue=np.float64(9.024229215374189e-12), stderr=np.float64(0.10830516572266864), intercept_stderr=np.float64(0.03237551409033611))\n\n\nWe see that the scipy.stats.linregress() method returns a five elements object. To make this method easier to use, we could generate five variables, define a function with those variables and plot a line based on this function :\n\n# We define the five variables from the linregress() method :\nslope, intercept, r, p, std_err = scipy.stats.linregress(x, y)\n\n# We define a function returning a linear function with the slope and the intercept :\ndef myfunc(x):\n  return slope * x + intercept\n\n# We create a list object with the list() function that takes as an argument a map object, based on our function and on the variable x :\nmymodel = list(map(myfunc, x))\nmymodel\n\n[np.float64(0.3466552962850944), np.float64(0.4855711228389237), np.float64(0.9147031170175356), np.float64(0.5153160299254063), np.float64(0.3801609990790563), np.float64(0.6044597742873725), np.float64(0.8592707674606043), np.float64(0.36509600545338833), np.float64(0.5527195718240848), np.float64(0.5240474317193224), np.float64(0.6014795199237659), np.float64(0.42070431481441095), np.float64(0.3281109864632415), np.float64(0.609338479700958), np.float64(0.5272774936883462), np.float64(0.49793336361061025), np.float64(0.6615576168832237), np.float64(0.48558369820808656), np.float64(0.682856174322561), np.float64(0.7340434195846064), np.float64(0.5627113232349656), np.float64(0.32278236816126854), np.float64(0.5929799171046213), np.float64(0.35278841098901403), np.float64(0.3580677536604607), np.float64(0.42191435405309263), np.float64(0.3238418122945764), np.float64(0.3191886742819142), np.float64(0.3568292775067544), np.float64(0.32757438236782), np.float64(0.32671680069225056), np.float64(0.42326584087775343), np.float64(0.3301253898513797), np.float64(0.49109681714494313), np.float64(0.44458311312708415), np.float64(0.5199585587348523), np.float64(0.9186022591677041), np.float64(0.39114282592853183), np.float64(0.4014183383452378), np.float64(0.4877972869989182), np.float64(0.3266773853988873), np.float64(0.4588845589667885), np.float64(0.5815321192975246), np.float64(0.5075743754991987), np.float64(0.46377760017945563), np.float64(0.3574396105070033), np.float64(0.39249417206726944), np.float64(0.488752927533466), np.float64(0.35569069788803687), np.float64(0.49822895229055497), np.float64(0.5161923540876256), np.float64(0.5088533951852636), np.float64(0.769034375640042), np.float64(0.6326334342149869), np.float64(1.0423321590360448), np.float64(0.3296848512011278), np.float64(0.3707922844586858), np.float64(0.4266797387634848), np.float64(0.33652341146276105), np.float64(0.39743971330310107), np.float64(0.3336311390693917), np.float64(0.5564567995066856), np.float64(0.376091444906386), np.float64(0.45090231896434735), np.float64(0.5040838302265498), np.float64(0.4051622523445217), np.float64(0.3226383239044324), np.float64(0.33191401608324494), np.float64(0.4666930634968537), np.float64(0.570707699127708), np.float64(0.5617582353940804), np.float64(0.3124068143973404), np.float64(0.5292769029282633), np.float64(0.3203018487632795), np.float64(0.7373582721485279), np.float64(0.36890927811192537), np.float64(0.5325929851471497), np.float64(0.6323717424690725), np.float64(0.932503917572861), np.float64(0.33708525383560667), np.float64(0.6526942091151531), np.float64(0.35715817364214664), np.float64(1.015109148788574), np.float64(0.32887962550451877), np.float64(0.7060221022682294), np.float64(0.339489680159918), np.float64(0.6722012746136157), np.float64(0.7056671726580602), np.float64(0.745471258289146), np.float64(0.4308133963682357), np.float64(0.33121240452941486), np.float64(0.37579615641214226), np.float64(0.40511754269860345), np.float64(0.40202405377269135), np.float64(0.7391566857584171), np.float64(0.5956863712448213), np.float64(0.4803225716002546), np.float64(0.3907379210536772), np.float64(0.4214984627624902), np.float64(0.3109604583787798)]\n\n\nThe definition of the variable mymodel is worth explaning a bit more in details. The list() function is used to create a list object (which is considered one of the most versatile objects in Python), like the c() function creates a vector in R. The map() function returns a map object, defined as :\n\nmap(func, *iterables) –&gt; map object Make an iterator that computes the function using arguments from each of the iterables. Stops when the shortest iterable is exhausted.\n\nin the Python documentation.\nNow, if we want to add this line to our plot, we can run :\n\nmatplotlib.pyplot.scatter(x,y)\nmatplotlib.pyplot.plot(x, mymodel)\n\n\n\n\n\n\n\n\nWhile it is curious that the variable mymodel is displayed as a continuous line, it works, so I am fine with this solution for now.\nIf we want only the more relevant information, we can choose to only work with the two first elements of the array, and plot it, we can do in a more concise way :\n\ndef f(x):\n    return scipy.stats.linregress(x, y)[1]+numpy.dot(x,scipy.stats.linregress(x, y)[0])\n\nfloat(scipy.stats.linregress(x, y)[1]),float(scipy.stats.linregress(x, y)[0])\n\n(0.30883317915377173, 0.8383718326044476)\n\nmatplotlib.pyplot.scatter(x,y)\nmatplotlib.pyplot.plot(x, f(x))\n\n\n\n\n\n\n\n\nWe note here that we are using the numpy.dot() method from the numpy library. This method allows us to do the dot product of two different matrixes. In our case, doing only x * scipy.stats.linregress(x,y)[0] would have returned an error, since x and the other term are not of the same dimension. This method thus allows to avoid this error by vectorizing the multiplication.\nNow that we covered a how to perform a simple linear regression with Python, let us move to a multiple linear regression."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#running-a-multiple-linear-regression",
    "href": "posts/2_regressions_python/index.html#running-a-multiple-linear-regression",
    "title": "Linear Regressions with Python",
    "section": "3.1. Running a multiple linear regression",
    "text": "3.1. Running a multiple linear regression\nFirst, let us generate some random data and fit our model.\nTo generate random data, we can do :\n\n# We randomly generate four variables of length 100 :\nw,x,y,z = (numpy.dot([random.random() for _ in range(100)],_) for _ in (1,2,3,4))\n# Just to check the length : \nlen(y)\n\n100\n\n\nTo run a multinomial linear regression that satisfies the OLS equation, we could use several libraries. Let us say that we want to explain the z variable by x and y.\n\n3.1.1. The sklearn library\nOne approach that seems to be widely used is to use the sklearn library. More specifically, we could use the sklearn.linear_model package :\n\nimport sklearn.linear_model\n\n# We create an object that regroups x, y and w :\nregressors = numpy.column_stack((x,y,w))\n\n# We create the object 'model' that has the LinearRegression() class from the sklearn.linear_model package : \nmodel = sklearn.linear_model.LinearRegression()\n \n# Fit the model to the data\nmodel.fit(regressors, z)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nmodel.coef_\n\narray([0.02372518, 0.22686716, 0.08769466])\n\nmodel.intercept_\n\nnp.float64(1.5537197507494207)\n\n\nThe model.intercept_ value is the intercept of z, and the coefficients are consistent with the regressors object. The first one corresponds to the effect of x, the second to the effect of y, and the last one to the effect of w.\n\n\n3.1.2. The statsmodels library\nTo get a nicer display and final object, we can alternatively use the statsmodels library.\n\nimport statsmodels.api\n\n# Create the object and specify an intercept :\nregressors = statsmodels.api.add_constant(regressors)\nmodel = statsmodels.api.OLS(z,regressors)\n\n# Run the model :\nresults = model.fit()\n\n# Print the regression table : \nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.030\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.9966\nDate:                Fri, 05 Jul 2024   Prob (F-statistic):              0.398\nTime:                        14:07:22   Log-Likelihood:                -152.78\nNo. Observations:                 100   AIC:                             313.6\nDf Residuals:                      96   BIC:                             324.0\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.5537      0.358      4.342      0.000       0.843       2.264\nx1             0.0237      0.202      0.118      0.907      -0.377       0.424\nx2             0.2269      0.135      1.684      0.095      -0.041       0.494\nx3             0.0877      0.400      0.219      0.827      -0.706       0.881\n==============================================================================\nOmnibus:                       17.960   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                5.139\nSkew:                           0.179   Prob(JB):                       0.0766\nKurtosis:                       1.949   Cond. No.                         9.43\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can also access to the itnercept and the coefficients directly, by doing :\n\nresults.params\n\narray([1.55371975, 0.02372518, 0.22686716, 0.08769466])"
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-a-static-3d-plot",
    "href": "posts/2_regressions_python/index.html#plotting-a-static-3d-plot",
    "title": "Linear Regressions with Python",
    "section": "3.2. Plotting a static 3D plot",
    "text": "3.2. Plotting a static 3D plot\nIf we now want to limit ourselves to a four dimensional problem, we can plot 3D graphs with colors representing the fourth dimension. In this section, we are first going to plot a static graph.\n\n# We first create the framework of a plot, that we name \"figure\" : \nfigure = matplotlib.pyplot.figure()\n\n# In this figure, we are going to use the add_subplot method to create a three dimensional projection : \nprojection = figure.add_subplot(projection='3d')\n\n# In this three dimensional space, we are going to project our points in a scatterplot way.\n# The argument 'c' stands for the color of the points :\nprojection.scatter(x,y,z, c = w)\n\n# Finally, we give each of our axis a name : \nprojection.set_xlabel(\"X\")\nprojection.set_ylabel(\"Y\")\nprojection.set_zlabel(\"Z\")\n\n\n\n\n\n\n\n\nThe syntax of this example can be a bit hard to understand at first, but is required when using matplotlib.pyplot library."
  },
  {
    "objectID": "posts/2_regressions_python/index.html#plotting-a-rotatable-3d-plot",
    "href": "posts/2_regressions_python/index.html#plotting-a-rotatable-3d-plot",
    "title": "Linear Regressions with Python",
    "section": "3.3. Plotting a rotatable 3D plot",
    "text": "3.3. Plotting a rotatable 3D plot\nNow it’s time for a rotatable plot, like in the previous article. Fortunately, the plotly library does also exist for Python, which is great, because I did not find any other library that yielded a similar result. However, to use plotly, we need the pandas library.\nWe can thus create a dataframe object from a dictionary with the four previously created vectors. If we only want to display a 3D scatter plot, we can run :\n\nimport plotly.express\nimport pandas\n\n# We create a four columns dataframe with our four previously generated vectors : \ndata = {'c1':[x],'c2':[y],'c3':[z],'c4':[w]}\nX = pandas.DataFrame(data)\n\nfig = plotly.express.scatter_3d(X, x,y,z, color=w)\nfig.show()\n\n                        \n                                            \n\n\nWe note that this yields a plot with similar features to the ones obtained in the previous article with Julia. The reason for that is that both obtained plots are done using the plotly library.\nFor the rest of the section, we are going to limit ourselves to only two regressands, excluding the w variable, since the display will be easier to understand this way :\n\nX = numpy.column_stack((x,y))\nX = statsmodels.api.add_constant(X)\nmodel = statsmodels.api.OLS(z,X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.030\nModel:                            OLS   Adj. R-squared:                  0.010\nMethod:                 Least Squares   F-statistic:                     1.485\nDate:                Fri, 05 Jul 2024   Prob (F-statistic):              0.232\nTime:                        14:07:23   Log-Likelihood:                -152.81\nNo. Observations:                 100   AIC:                             311.6\nDf Residuals:                      97   BIC:                             319.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.5915      0.312      5.099      0.000       0.972       2.211\nx1             0.0273      0.200      0.136      0.892      -0.370       0.425\nx2             0.2292      0.134      1.715      0.090      -0.036       0.494\n==============================================================================\nOmnibus:                       18.620   Durbin-Watson:                   2.015\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                5.252\nSkew:                           0.186   Prob(JB):                       0.0724\nKurtosis:                       1.941   Cond. No.                         6.97\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNow, we need more lines to plot a surface. More specifically, we are going to use plotly.graph_objects and make use of the meshgrid method :\n\nimport plotly.graph_objects\n\n# We first store the information of our regression : \nintercept, coef1, coef2 = results.params\n\n# We then create meshgrids for x,y, and z : \nx_range = numpy.linspace(x.min(), x.max(), 10)\ny_range = numpy.linspace(y.min(), y.max(), 10)\nx_grid, y_grid = numpy.meshgrid(x_range, y_range)\nz_grid = intercept + coef1 * x_grid + coef2 * y_grid\n\n# We now generate a surface object with the grids we just created\nsurface = plotly.graph_objects.Surface(\n    x=x_grid, y=y_grid, z=z_grid,\n    colorscale='Viridis', opacity=0.5\n)\n\n# We create a scatter object, to be able to plot it with the surface :\nscatter = plotly.graph_objects.Scatter3d(\n    x=x, y=y, z=z,\n    mode='markers',\n    marker=dict(size=5, color=w, colorscale='Viridis')\n)\n\n# We create a figure object, with the scatter and surface tha we just generated : \nfig = plotly.graph_objects.Figure(data = [scatter, surface])\n\n# We display the figure : \nfig.show()\n\n                        \n                                            \n\n\nThis looks satisfactory. I even manage to plot the points with the surface of the regression in the 3D plot."
  }
]